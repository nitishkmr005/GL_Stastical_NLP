{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4. Test Processing & Web Data Retrieval.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["ZPCzgz2ZwySW"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qgy1Jzg-wyRp","colab_type":"text"},"source":["### Web data retrieval"]},{"cell_type":"code","metadata":{"id":"-Z1V23urwyRr","colab_type":"code","colab":{}},"source":["import requests\n","import textwrap"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Chgw6qBTwyRv","colab_type":"code","colab":{}},"source":["#Let's download the data from inshorts website. In this case, news articles will be from 'technolgy' category\n","url = 'https://inshorts.com/en/read/technology'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"21pavZKVwyR1","colab_type":"code","colab":{}},"source":["news_category = url.split('/')[-1]\n","news_category"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gqanoc8o1NCn","colab_type":"text"},"source":["Download HTML data"]},{"cell_type":"code","metadata":{"id":"q9xSbV5BwyR-","colab_type":"code","colab":{}},"source":["#Download the data from Website\n","data = requests.get(url)\n","data.content"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dVJ8LqgK1Q4w","colab_type":"text"},"source":["### Data Cleaning"]},{"cell_type":"markdown","metadata":{"id":"NNu23gpBGJxm","colab_type":"text"},"source":["We can use Beautiful Soup package to clean Web data"]},{"cell_type":"code","metadata":{"id":"K87WWgtuGIcL","colab_type":"code","colab":{}},"source":["from bs4 import BeautifulSoup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZ8gHfTBwySC","colab_type":"code","colab":{}},"source":["soup = BeautifulSoup(data.content, 'html.parser')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4wI2Dqez5Xh","colab_type":"code","colab":{}},"source":["soup"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mi3FrilR1ZbH","colab_type":"text"},"source":["Read all the articles. For each article, we will read:\n","\n","1. Headline\n","2. Article body\n","3. Category\n","\n","This is done by reading text between specific HTML tags. The tags depend on actual web page"]},{"cell_type":"code","metadata":{"id":"Y6NOaNg-wyRy","colab_type":"code","colab":{}},"source":["news_data = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcmvcnDfwySF","colab_type":"code","colab":{}},"source":["news_articles = [{'news_headline': headline.find('span', attrs={'itemprop': 'headline'}).string,\n","                  'news_article': article.find('div', attrs={'itemprop': 'articleBody'}).string,\n","                  'news_category': news_category} \n","                 for headline, article in zip(soup.find_all('div', \n","                                                            class_ = ['news-card-title news-right-box']), \n","                                              soup.find_all('div', class_=['news-card-content news-right-box']))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkrtuxs2wySJ","colab_type":"code","colab":{}},"source":["#Check news data\n","news_data.extend(news_articles)\n","news_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pEnaJDntGY15","colab_type":"text"},"source":["Read the news data in a Dataframe"]},{"cell_type":"code","metadata":{"id":"ZPlHoL7GGcgk","colab_type":"code","colab":{}},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GjsAGXcEwySN","colab_type":"code","colab":{}},"source":["#Building dataframe\n","df = pd.DataFrame(news_data, columns=['news_headline', 'news_article', 'news_category'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QPebjVhOwyST","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZPCzgz2ZwySW","colab_type":"text"},"source":["### Extract multiple categories\n","\n","Function to extract data from inshorts.com. The function will:\n","\n","1. take a URLs list as input\n","2. Get content for each URL\n","3. Extract news article headline, body and category"]},{"cell_type":"code","metadata":{"id":"XfwI4DfwwySX","colab_type":"code","colab":{}},"source":["urls_list = ['https://inshorts.com/en/read/technology',\n","             'https://inshorts.com/en/read/sports',\n","             'https://inshorts.com/en/read/world']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jitD63HQwySa","colab_type":"code","colab":{}},"source":["def datasetPrepare(urls_list):\n","    \n","    news_data = []\n","    for url in urls_list:\n","        news_category = url.split('/')[-1]\n","        data = requests.get(url)\n","        soup = BeautifulSoup(data.content, 'html.parser')\n","        news_articles = [{'news_headline': headline.find('span', attrs={\"itemprop\": \"headline\"}).string,\n","                          'news_article': article.find('div', attrs={\"itemprop\": \"articleBody\"}).string,\n","                          'news_category': news_category}\n","                         \n","                            for headline, article in \n","                             zip(soup.find_all('div', class_=[\"news-card-title news-right-box\"]),\n","                                 soup.find_all('div', class_=[\"news-card-content news-right-box\"]))\n","                        ]\n","        news_data.extend(news_articles) \n","    df =  pd.DataFrame(news_data)\n","    df = df[['news_headline', 'news_article', 'news_category']]\n","    return df    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruY2Z-DIwySd","colab_type":"code","colab":{}},"source":["#Build the dataframe\n","news_df = datasetPrepare(urls_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCfWltFHwySm","colab_type":"code","colab":{}},"source":["news_df.sample(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rXVQKGNwySp","colab_type":"code","colab":{}},"source":["#Articles count by category\n","news_df.news_category.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NBK3ZgwUwySt","colab_type":"text"},"source":["# Text Wrangling and Pre-processing"]},{"cell_type":"code","metadata":{"id":"tPQmV8DiJJF1","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXQNHEHpJKdf","colab_type":"code","colab":{}},"source":["import re\n","import unicodedata\n","from nltk.stem import WordNetLemmatizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wh9CrhePwyS9","colab_type":"text"},"source":["## Remove HTML tags"]},{"cell_type":"code","metadata":{"id":"1Xit3_ckwyS9","colab_type":"code","colab":{}},"source":["def strip_html_tags(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    stripped_text = soup.get_text()\n","    return stripped_text\n","\n","strip_html_tags('<html><h2>Some important text</h2></html>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HBbRSoPkwyTB","colab_type":"text"},"source":["## Remove accented characters"]},{"cell_type":"code","metadata":{"id":"TOBdIlTWwyTB","colab_type":"code","colab":{}},"source":["def remove_accented_chars(text):\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return text\n","\n","remove_accented_chars('Sómě Áccěntěd těxt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8u9SuUXOwyTM","colab_type":"text"},"source":["## Remove special characters"]},{"cell_type":"code","metadata":{"id":"lCmBJeI5wyTN","colab_type":"code","colab":{}},"source":["def remove_special_characters(text, remove_digits=False):\n","    #Using regex\n","    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n","    text = re.sub(pattern, '', text)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9IGXQ-LwyTQ","colab_type":"code","colab":{}},"source":["remove_special_characters(\"Well this was fun! What do you think? 123#@!\", remove_digits=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ryt_VSOVwyTT","colab_type":"text"},"source":["## Text lemmatization"]},{"cell_type":"code","metadata":{"id":"uNR0fNT4wyTU","colab_type":"code","colab":{}},"source":["def lemmatize_text(text):\n","\n","    lemmatizer = WordNetLemmatizer()\n","    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"do-LTxKRwyTX","colab_type":"code","colab":{}},"source":["lemmatize_text(\"My system keeps crashing, his crashed yesterday, ours crashes daily\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWmDYl9swyTa","colab_type":"text"},"source":["## Text stemming"]},{"cell_type":"code","metadata":{"id":"wcgdJFzLwyTb","colab_type":"code","colab":{}},"source":["def simple_stemmer(text):\n","    ps = nltk.porter.PorterStemmer()\n","    text = ' '.join([ps.stem(word) for word in text.split()])\n","    return text\n","\n","simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S8AdJuUuwyTq","colab_type":"text"},"source":["## Building a text normalizer"]},{"cell_type":"code","metadata":{"id":"brUdlUOJwyTu","colab_type":"code","colab":{}},"source":["def normalize_corpus(corpus, html_stripping=True, accented_char_removal=True, text_lower_case=True, \n","                     text_lemmatization=True, special_char_removal=True, \n","                     stopword_removal=True, remove_digits=True):\n","    \n","    normalized_corpus = []\n","    # normalize each document in the corpus\n","    for doc in corpus:\n","        # strip HTML\n","        if html_stripping:\n","            doc = strip_html_tags(doc)\n","        # remove accented characters\n","        if accented_char_removal:\n","            doc = remove_accented_chars(doc)\n","        # lowercase the text    \n","        if text_lower_case:\n","            doc = doc.lower()\n","        # remove extra newlines\n","        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n","        # lemmatize text\n","        if text_lemmatization:\n","            doc = lemmatize_text(doc)\n","        # remove special characters and\\or digits    \n","        if special_char_removal:\n","            # insert spaces between special characters to isolate them    \n","            special_char_pattern = re.compile(r'([{.(-)!}])')\n","            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n","            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n","        # remove extra whitespace\n","        doc = re.sub(' +', ' ', doc)\n","            \n","        normalized_corpus.append(doc)\n","        \n","    return normalized_corpus"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2c5hDSNwyTw","colab_type":"text"},"source":["## Pre-process and normalize news articles"]},{"cell_type":"code","metadata":{"id":"1da9VaH5wyTx","colab_type":"code","colab":{}},"source":["news_df['full_text'] = news_df[\"news_headline\"].map(str)+ '. ' + news_df[\"news_article\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVZRnQVBwyTz","colab_type":"code","colab":{}},"source":["news_df['clean_text'] = normalize_corpus(news_df['full_text'])\n","norm_corpus = list(news_df['clean_text'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzEFnNeuJlIo","colab_type":"code","colab":{}},"source":["news_df.iloc[1][['full_text', 'clean_text']].to_dict()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MpmBJJXhwyT2","colab_type":"text"},"source":["# Save the news articles"]},{"cell_type":"code","metadata":{"id":"NLVKFz3RwyT3","colab_type":"code","colab":{}},"source":["news_df.to_csv('news.csv', index=False, encoding='utf-8')"],"execution_count":null,"outputs":[]}]}