{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"4c. Sentiment_Analysis_Glove_Embedding.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["mQKm7K78KMQa","40sSeDoWKMQx","2GgPOuSzKMRA","fKmVWM5pKMRF","m7CMlSVYCHNA","jzfbzZUQRujx","UTZqcuaqB2cl","wAOvV9C_KMRl","j_aH5TX5KMSA","2NvjDJo7OYOb"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mQKm7K78KMQa","colab_type":"text"},"source":["#### Load Movie reviews Dataset"]},{"cell_type":"markdown","metadata":{"id":"IS0axMJDKrph","colab_type":"text"},"source":["We will be using data available on Kaggle platform for this exercise. The data is available at https://www.kaggle.com/c/word2vec-nlp-tutorial/data."]},{"cell_type":"code","metadata":{"id":"WxnI1KLhLJ_J","colab_type":"code","colab":{}},"source":["#Connect Google drive to colab\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDssadQzJnmz","colab_type":"text"},"source":["Load dataset"]},{"cell_type":"code","metadata":{"id":"1Y_ohNjGKt6R","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pb3r_KaeJJ0d","colab_type":"code","colab":{}},"source":["#change file path to point to where you have stored the zip file.\n","df = pd.read_csv('/gdrive/My Drive/AI-ML/labeledTrainData.tsv.zip', header=0, delimiter=\"\\t\", quoting=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l70lgDrNJZ-F","colab_type":"code","colab":{}},"source":["print('Number of examples in Dataset: ', df.shape)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q12_oUDIIC-k","colab_type":"code","colab":{}},"source":["df.loc[0, 'review']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45uxRF4rKMQq","colab_type":"text"},"source":["Split Data into Training and Test Data"]},{"cell_type":"code","metadata":{"id":"cwNZ5XEpKMQq","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-RNaPq2KMQt","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    df['review'],\n","    df['sentiment'],\n","    test_size=0.2, \n","    random_state=42\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IdMWitcdByBs","colab_type":"code","colab":{}},"source":["X_train.shape, X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40sSeDoWKMQx","colab_type":"text"},"source":["#### Build the Tokenizer"]},{"cell_type":"code","metadata":{"id":"AGM55RRUM3fN","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNB6T0EVKMQ6","colab_type":"code","colab":{}},"source":["desired_vocab_size = 10000 #Vocablury size\n","t = tf.keras.preprocessing.text.Tokenizer(num_words=desired_vocab_size, oov_token=32) # num_words -> Vocablury size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t65mfe_2KMQ8","colab_type":"code","colab":{}},"source":["#Fit tokenizer with actual training data\n","t.fit_on_texts(X_train.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6N7cgYEvVGzB","colab_type":"code","colab":{}},"source":["#Vocabulary\n","t.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2GgPOuSzKMRA","colab_type":"text"},"source":["#### Prepare Training and Test Data"]},{"cell_type":"markdown","metadata":{"id":"8o8fG3FtKMRA","colab_type":"text"},"source":["Get the word index for each of the word in the review"]},{"cell_type":"code","metadata":{"id":"G9m65RFCVXCd","colab_type":"code","colab":{}},"source":["X_train[0:1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNQIpYPKKMRB","colab_type":"code","colab":{}},"source":["X_train = t.texts_to_sequences(X_train.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xh1nDZFDVlB8","colab_type":"code","colab":{}},"source":["print(X_train[0:1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Gix3lNmKMRD","colab_type":"code","colab":{}},"source":["X_test = t.texts_to_sequences(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hb9z28TZKMRF","colab_type":"text"},"source":["How many words in each review?"]},{"cell_type":"code","metadata":{"id":"O7maQ5kpxdfI","colab_type":"code","colab":{}},"source":["len(X_train[200])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fKmVWM5pKMRF","colab_type":"text"},"source":["#### Pad Sequences - Important"]},{"cell_type":"code","metadata":{"id":"h5YfEUx2KMRI","colab_type":"code","colab":{}},"source":["#Define maximum number of words to consider in each review\n","max_review_length = 300"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeJeFjogKMRM","colab_type":"code","colab":{}},"source":["#Pad training and test reviews\n","X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n","                                                        maxlen=max_review_length,\n","                                                        padding='pre')\n","X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, \n","                                                       maxlen=max_review_length, \n","                                                       padding='pre')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4GLCWBOlztU","colab_type":"code","colab":{}},"source":["X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QJqX-Z5wL-W","colab_type":"code","colab":{}},"source":["X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVteHr5IzS4I","colab_type":"code","colab":{}},"source":["X_train[200]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7CMlSVYCHNA","colab_type":"text"},"source":["#### Download Glove model\n","\n","Check out this [link](https://nlp.stanford.edu/projects/glove/) for available pre-trained Glove models from Standord."]},{"cell_type":"code","metadata":{"id":"t9fS8hLYCLiD","colab_type":"code","colab":{}},"source":["#This will take few minutes\n","!wget http://nlp.stanford.edu/data/glove.6B.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOTPKU69CM5_","colab_type":"code","colab":{}},"source":["#Check if embeddings have been downloaded\n","!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sq7qgQQGCUwZ","colab_type":"code","colab":{}},"source":["#unzip the file, we get multiple embedding files. We can use either one of them\n","!unzip glove.6B.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a5tBMpztRpiL","colab_type":"text"},"source":["We have 4 models in this zip file. '50d' in the filename means that embedding size is 50 (100d means embedding size 100). We can use any one of the 4 models."]},{"cell_type":"code","metadata":{"id":"KeGsdG22CZWT","colab_type":"code","colab":{}},"source":["!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jzfbzZUQRujx","colab_type":"text"},"source":["#### Convert Glove to Word2Vec format"]},{"cell_type":"code","metadata":{"id":"M9pWCldqRxkS","colab_type":"code","colab":{}},"source":["#Install Gensim\n","!pip install gensim --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6cDJb-RR0nn","colab_type":"code","colab":{}},"source":["from gensim.scripts.glove2word2vec import glove2word2vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3DNgLyLgR3H_","colab_type":"code","colab":{}},"source":["#Glove file - we are using model with 50 embedding size\n","glove_input_file = 'glove.6B.50d.txt'\n","\n","#Name for word2vec file\n","word2vec_output_file = 'glove.6B.50d.txt.word2vec'\n","\n","#Convert Glove embeddings to Word2Vec embeddings\n","glove2word2vec(glove_input_file, word2vec_output_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lewBKMuRR5W5","colab_type":"code","colab":{}},"source":["!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UTZqcuaqB2cl","colab_type":"text"},"source":["#### Get Pre-trained Embeddings"]},{"cell_type":"markdown","metadata":{"id":"n3Ftgb1zSDvG","colab_type":"text"},"source":["Pre-trained Glove model has 400,000 unique words (Vocabulary size). We do not need all the words. Moreover, we have to arrange word embeddings according to word index created by our tokenizers above. So we will extract word embeddings for only the words that we are interested in."]},{"cell_type":"code","metadata":{"id":"uCKYf3oxSIS4","colab_type":"code","colab":{}},"source":["from gensim.models import Word2Vec, KeyedVectors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tnfmU0U7B1ja","colab_type":"code","colab":{}},"source":["# Load pretrained Glove model (in word2vec form)\n","glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbgVAzAPDl1O","colab_type":"code","colab":{}},"source":["#Embedding length based on selected model - we are using 50d here.\n","embedding_vector_length = 50"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBQOS906KX0p","colab_type":"text"},"source":["Initialize a embedding matrix which we will populate for our vocabulary words."]},{"cell_type":"code","metadata":{"id":"HPNcZC9PEA8v","colab_type":"code","colab":{}},"source":["#Initialize embedding matrix for our dataset with 10000+1 rows (1 for padding word)\n","#and 50 columns (as embedding size is 50)\n","embedding_matrix = np.zeros((desired_vocab_size + 1, embedding_vector_length))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SVaLTX2cTZ79","colab_type":"text"},"source":["Load word vectors for each word in our vocabulary from from Glove pre-trained model"]},{"cell_type":"code","metadata":{"id":"mufDrkM-EKlK","colab_type":"code","colab":{}},"source":["for word, i in sorted(t.word_index.items(),key=lambda x:x[1]):\n","    if i > (desired_vocab_size+1):\n","        break\n","    try:\n","        embedding_vector = glove_model[word] #Reading word's embedding from Glove model for a given word\n","        embedding_matrix[i] = embedding_vector\n","    except:\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ww5J3x_LGg3","colab_type":"text"},"source":["We now have word embeddings for our vocabulary words from Glove model. We can now use it in our Model training."]},{"cell_type":"code","metadata":{"id":"qeZNhU2ZEs1w","colab_type":"code","colab":{}},"source":["#embedding_matrix[2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wAOvV9C_KMRl","colab_type":"text"},"source":["#### Build Model - Dense Layers"]},{"cell_type":"code","metadata":{"id":"pWtUZzM3KMRs","colab_type":"code","colab":{}},"source":["#Initialize model\n","tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rbo6eeNGMtWn","colab_type":"text"},"source":["To handle, pre-trained embeddings, we will use Keras Embedding layer"]},{"cell_type":"code","metadata":{"id":"vUTG9uAMM-z3","colab_type":"code","colab":{}},"source":["model.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n","                                    embedding_vector_length, #Embedding size\n","                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n","                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n","                                    input_length=max_review_length) #Number of words in each review\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKyc5UQSMDQG","colab_type":"text"},"source":["Embedding Layer gives us 3D output ->\n","[Batch_Size , Review Length , Embedding_Size]"]},{"cell_type":"code","metadata":{"id":"ezX7QcD8NSmw","colab_type":"code","colab":{}},"source":["model.output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DF_wJp6sKMRv","colab_type":"text"},"source":["Add Hidden layers"]},{"cell_type":"code","metadata":{"id":"iignS5XqKMRv","colab_type":"code","colab":{}},"source":["#Flatten the data as we will use Dense layers\n","model.add(tf.keras.layers.Flatten())\n","\n","#Add Hidden layers (Dense layers)\n","model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=()))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Dense(50, activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Dense(25, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.25))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8ua5Dj8LVk2","colab_type":"text"},"source":["Add Output layer"]},{"cell_type":"code","metadata":{"id":"9esFq2mNZfFZ","colab_type":"code","colab":{}},"source":["model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GobXBLHXKMR9","colab_type":"code","colab":{}},"source":["#Compile the model\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iueK1G3pOznW","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j_aH5TX5KMSA","colab_type":"text"},"source":["##### Train Model"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"AV3TceqjKMSC","colab_type":"code","colab":{}},"source":["model.fit(X_train,y_train,\n","          epochs=5,\n","          batch_size=32,          \n","          validation_data=(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NvjDJo7OYOb","colab_type":"text"},"source":["#### Building a CNN Model"]},{"cell_type":"markdown","metadata":{"id":"PlhUrG_EO2Ga","colab_type":"text"},"source":["Start a model"]},{"cell_type":"code","metadata":{"id":"tc0ZANd3OaWs","colab_type":"code","colab":{}},"source":["model2 = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRX6YC7_O3to","colab_type":"text"},"source":["Add Embedding layer to handle Word2Vec"]},{"cell_type":"code","metadata":{"id":"gF2fufQFOx2w","colab_type":"code","colab":{}},"source":["model2.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n","                                    embedding_vector_length, #Embedding size\n","                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n","                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n","                                    input_length=max_review_length) #Number of words in each review\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJRM3ICZO8lD","colab_type":"text"},"source":["Add Conv1D hidden layers : As our text data is 2D (number of words, Embedding size), we will use Conv1D in this case (compared to Conv2D with images which are 3D)"]},{"cell_type":"code","metadata":{"id":"3a03h-_9OmqL","colab_type":"code","colab":{}},"source":["#Add first convolutional layer\n","model2.add(tf.keras.layers.Conv1D(32, #Number of filters \n","                                 kernel_size=(3), #Size of the filter\n","                                 strides=1,\n","                                 activation='relu'))\n","\n","#normalize data\n","model2.add(tf.keras.layers.BatchNormalization())\n","\n","#Add second convolutional layer\n","model2.add(tf.keras.layers.Conv1D(64, kernel_size=(3), strides=2))\n","model2.add(tf.keras.layers.ReLU())\n","\n","#normalize data\n","model2.add(tf.keras.layers.BatchNormalization())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKJbqsCNPTLr","colab_type":"code","colab":{}},"source":["#Use Global Average Pooling\n","model2.add(tf.keras.layers.GlobalAveragePooling1D())\n","\n","#Output layer\n","model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZCe3ghEPjyx","colab_type":"code","colab":{}},"source":["#Compile the model\n","model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ekq97ri0Pne2","colab_type":"code","colab":{}},"source":["model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0w0oGtKpPxkp","colab_type":"code","colab":{}},"source":["model2.fit(X_train,y_train,\n","          epochs=5,\n","          batch_size=32,          \n","          validation_data=(X_test, y_test))"],"execution_count":null,"outputs":[]}]}